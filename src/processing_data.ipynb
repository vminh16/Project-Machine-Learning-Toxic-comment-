{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e164bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng d√≤ng: 164069\n",
      "\n",
      "C√°c c·ªôt: ['text', 'label', 'source', 'language']\n",
      "\n",
      "Th√¥ng tin chi ti·∫øt:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164069 entries, 0 to 164068\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   text      164069 non-null  object\n",
      " 1   label     164069 non-null  int64 \n",
      " 2   source    164069 non-null  object\n",
      " 3   language  164069 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load d·ªØ li·ªáu\n",
    "df = pd.read_csv('../data/merged_dataset.csv')\n",
    "\n",
    "# Xem th√¥ng tin c∆° b·∫£n\n",
    "print(f\"S·ªë l∆∞·ª£ng d√≤ng: {len(df)}\")\n",
    "print(f\"\\nC√°c c·ªôt: {df.columns.tolist()}\")\n",
    "print(f\"\\nTh√¥ng tin chi ti·∫øt:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b50d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  source language\n",
      "0  Explanation\\nWhy the edits made under my usern...      0  jigsaw       en\n",
      "1  D'aww! He matches this background colour I'm s...      0  jigsaw       en\n",
      "2  Hey man, I'm really not trying to edit war. It...      0  jigsaw       en\n",
      "3  \"\\nMore\\nI can't make any real suggestions on ...      0  jigsaw       en\n",
      "4  You, sir, are my hero. Any chance you remember...      0  jigsaw       en\n",
      "                                                     text  label  source  \\\n",
      "47398   \"\\n\\nEach in turn\\n\\nSuggested: In physics, fo...      0  jigsaw   \n",
      "103425                 REDIRECT Talk:Pedro Daniel Estrada      0  jigsaw   \n",
      "127904  Why to go buddy! So how is Nancy holding up?\\n...      1  jigsaw   \n",
      "70185   \"\\n No, not phosphorescent. Phosphorescence, a...      0  jigsaw   \n",
      "36426   HELP \\n\\nHEY BUDDY I DON'T LIKE AT ALL!!!  \\n\\...      0  jigsaw   \n",
      "\n",
      "       language  \n",
      "47398        en  \n",
      "103425       en  \n",
      "127904       en  \n",
      "70185        en  \n",
      "36426        en  \n",
      "\n",
      "S·ªë l∆∞·ª£ng comments ti·∫øng Vi·ªát: 4498\n",
      "                                                     text  label   source  \\\n",
      "161780  Thuong la 1 ten qua toi ky luat qua nhe , can ...      0  youtube   \n",
      "163297  Di·ªÖn th√¨ do m√† cu doi co hao quang n√†y no r·ªìi ...      1  youtube   \n",
      "163152  Binz r·∫•t l√† l√†nh t√≠nh, ƒë√≥n nh·∫≠n m·ªçi ƒëi·ªÅu r·∫•t n...      0  youtube   \n",
      "\n",
      "       language  \n",
      "161780       vi  \n",
      "163297       vi  \n",
      "163152       vi  \n"
     ]
    }
   ],
   "source": [
    "# Xem 5 d√≤ng ƒë·∫ßu\n",
    "print(df.head())\n",
    "\n",
    "# Xem ng·∫´u nhi√™n 5 d√≤ng\n",
    "print(df.sample(5))\n",
    "\n",
    "# Xem ri√™ng d·ªØ li·ªáu ti·∫øng Vi·ªát\n",
    "vietnamese_df = df[df['language'] == 'vi']\n",
    "print(f\"\\nS·ªë l∆∞·ª£ng comments ti·∫øng Vi·ªát: {len(vietnamese_df)}\")\n",
    "print(vietnamese_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0222c88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng null trong m·ªói c·ªôt:\n",
      "text        0\n",
      "label       0\n",
      "source      0\n",
      "language    0\n",
      "dtype: int64\n",
      "\n",
      "S·ªë d√≤ng c√≥ text null: 0\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra null\n",
    "print(\"S·ªë l∆∞·ª£ng null trong m·ªói c·ªôt:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Ki·ªÉm tra chi ti·∫øt text null\n",
    "null_texts = df[df['text'].isnull()]\n",
    "print(f\"\\nS·ªë d√≤ng c√≥ text null: {len(null_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ac0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë comments tr·ªëng: 0\n",
      "Sau khi lo·∫°i text tr·ªëng: 164069 d√≤ng\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra text r·ªóng (ch·ªâ c√≥ kho·∫£ng tr·∫Øng)\n",
    "empty_texts = df[df['text'].str.strip() == '']\n",
    "print(f\"S·ªë comments tr·ªëng: {len(empty_texts)}\")\n",
    "\n",
    "# Lo·∫°i b·ªè\n",
    "df = df[df['text'].str.strip() != '']\n",
    "print(f\"Sau khi lo·∫°i text tr·ªëng: {len(df)} d√≤ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9805fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ƒê·ªô d√†i ng·∫Øn nh·∫•t: 6\n",
      "ƒê·ªô d√†i d√†i nh·∫•t: 5000\n",
      "ƒê·ªô d√†i trung b√¨nh: 385.9\n",
      "\n",
      "S·ªë comments < 5 k√Ω t·ª±: 0\n",
      "\n",
      "V√≠ d·ª•:\n",
      "Series([], Name: text, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "# Xem ph√¢n b·ªë ƒë·ªô d√†i\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(f\"\\nƒê·ªô d√†i ng·∫Øn nh·∫•t: {df['text_length'].min()}\")\n",
    "print(f\"ƒê·ªô d√†i d√†i nh·∫•t: {df['text_length'].max()}\")\n",
    "print(f\"ƒê·ªô d√†i trung b√¨nh: {df['text_length'].mean():.1f}\")\n",
    "\n",
    "# Xem nh·ªØng comments qu√° ng·∫Øn\n",
    "very_short = df[df['text_length'] < 5]\n",
    "print(f\"\\nS·ªë comments < 5 k√Ω t·ª±: {len(very_short)}\")\n",
    "print(\"\\nV√≠ d·ª•:\")\n",
    "print(very_short['text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd8ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sau khi lo·∫°i comments < 10 k√Ω t·ª±: 164064 d√≤ng\n"
     ]
    }
   ],
   "source": [
    "# Quy·∫øt ƒë·ªãnh threshold (v√≠ d·ª•: >= 10 k√Ω t·ª±)\n",
    "MIN_LENGTH = 10\n",
    "df = df[df['text_length'] >= MIN_LENGTH]\n",
    "print(f\"Sau khi lo·∫°i comments < {MIN_LENGTH} k√Ω t·ª±: {len(df)} d√≤ng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea73717e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë comments b·ªã duplicate: 0\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra duplicates\n",
    "duplicates = df.duplicated(subset=['text'])\n",
    "print(f\"S·ªë comments b·ªã duplicate: {duplicates.sum()}\")\n",
    "\n",
    "# Xem v√≠ d·ª•\n",
    "if duplicates.sum() > 0:\n",
    "    dup_examples = df[df.duplicated(subset=['text'], keep=False)].sort_values('text')\n",
    "    print(\"\\nV√≠ d·ª• duplicates:\")\n",
    "    print(dup_examples[['text', 'label', 'source']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bf39c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T·ªïng s·ªë d√≤ng cu·ªëi c√πng: 164064\n"
     ]
    }
   ],
   "source": [
    "# Reset index sau khi ƒë√£ x√≥a nhi·ªÅu d√≤ng\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"\\nT·ªïng s·ªë d√≤ng cu·ªëi c√πng: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76cbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc khi lowercase:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "\n",
      "Sau khi lowercase:\n",
      "explanation\n",
      "why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "# Xem tr∆∞·ªõc v√† sau\n",
    "print(\"Tr∆∞·ªõc khi lowercase:\")\n",
    "print(df['text'].iloc[0])\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "print(\"\\nSau khi lowercase:\")\n",
    "print(df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96acc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: xem video n√†y http://youtube.com/abc hay l·∫Øm\n",
      "Sau: xem video n√†y  hay l·∫Øm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# H√†m lo·∫°i b·ªè URLs\n",
    "def remove_urls(text):\n",
    "    # Pattern cho URLs\n",
    "    url_pattern = r'http\\S+|www\\.\\S+|https\\S+'\n",
    "    text = re.sub(url_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Test tr∆∞·ªõc\n",
    "test_text = \"xem video n√†y http://youtube.com/abc hay l·∫Øm\"\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {remove_urls(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f313d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·ªâ √°p d·ª•ng cho ti·∫øng Vi·ªát\n",
    "vietnamese_mask = df['language'] == 'vi'\n",
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24801d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: @admin xin ch√†o @user123 b·∫°n ∆°i\n",
      "Sau:  xin ch√†o  b·∫°n ∆°i\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    # Pattern: @ theo sau b·ªüi k√Ω t·ª± ch·ªØ/s·ªë\n",
    "    mention_pattern = r'@\\w+'\n",
    "    text = re.sub(mention_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "test_text = \"@admin xin ch√†o @user123 b·∫°n ∆°i\"\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {remove_mentions(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab71a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dacab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: b√†i n√†y #toxic #spam qu√°\n",
      "Sau: b√†i n√†y toxic spam qu√°\n"
     ]
    }
   ],
   "source": [
    "def process_hashtags(text):\n",
    "    # Lo·∫°i b·ªè # nh∆∞ng GI·ªÆ text\n",
    "    # #toxic ‚Üí toxic\n",
    "    hashtag_pattern = r'#(\\w+)'\n",
    "    text = re.sub(hashtag_pattern, r'\\1', text)\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "test_text = \"b√†i n√†y #toxic #spam qu√°\"\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {process_hashtags(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a629144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(process_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a008bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: li√™n h·ªá t√¥i qua email abc@gmail.com nh√©\n",
      "Sau: li√™n h·ªá t√¥i qua email  nh√©\n"
     ]
    }
   ],
   "source": [
    "def remove_emails(text):\n",
    "    email_pattern = r'\\S+@\\S+\\.\\S+'\n",
    "    text = re.sub(email_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "test_text = \"li√™n h·ªá t√¥i qua email abc@gmail.com nh√©\"\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {remove_emails(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fbdf657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(remove_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5206ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: 'text   n√†y    c√≥     nhi·ªÅu      spaces'\n",
      "Sau: 'text n√†y c√≥ nhi·ªÅu spaces'\n"
     ]
    }
   ],
   "source": [
    "def normalize_whitespace(text):\n",
    "    # Thay nhi·ªÅu spaces b·∫±ng 1 space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Lo·∫°i b·ªè space ƒë·∫ßu/cu·ªëi\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "test_text = \"text   n√†y    c√≥     nhi·ªÅu      spaces\"\n",
    "print(f\"Tr∆∞·ªõc: '{test_text}'\")\n",
    "print(f\"Sau: '{normalize_whitespace(test_text)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "651a10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(normalize_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe3c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: waaaaaa ƒë·∫πpppppp qu√°√°√°√°√°√°\n",
      "Sau: waa ƒë·∫πpp qu√°√°\n"
     ]
    }
   ],
   "source": [
    "def remove_repeated_chars(text):\n",
    "    # \"hayyyyyyy\" ‚Üí \"hayy\" (gi·ªØ t·ªëi ƒëa 2 k√Ω t·ª± l·∫∑p)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "# Test\n",
    "test_text = \"waaaaaa ƒë·∫πpppppp qu√°√°√°√°√°√°\"\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {remove_repeated_chars(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3c03dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(remove_repeated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aa61b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def remove_emoji_v2(text):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè emoji s·ª≠ d·ª•ng th∆∞ vi·ªán emoji\n",
    "    Ph∆∞∆°ng ph√°p n√†y to√†n di·ªán h∆°n regex\n",
    "    \"\"\"\n",
    "    # Thay th·∫ø t·∫•t c·∫£ emoji b·∫±ng chu·ªói r·ªóng\n",
    "    return emoji.replace_emoji(text, replace='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100a1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(remove_emoji_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06ae2b",
   "metadata": {},
   "source": [
    "X·ª≠ l√Ω teecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "794dc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teencode_dict = {\n",
    "    # Ph·ªß ƒë·ªãnh\n",
    "    'ko': 'kh√¥ng', 'k': 'kh√¥ng', 'khong': 'kh√¥ng', 'hok': 'kh√¥ng',\n",
    "    'kh': 'kh√¥ng', 'hong': 'kh√¥ng', 'h√¥ng': 'kh√¥ng',\n",
    "    \n",
    "    # ƒê·ªìng √Ω\n",
    "    'ok': 'ƒë∆∞·ª£c', 'oke': 'ƒë∆∞·ª£c', 'okie': 'ƒë∆∞·ª£c', 'okela': 'ƒë∆∞·ª£c',\n",
    "    'dc': 'ƒë∆∞·ª£c', 'ƒëc': 'ƒë∆∞·ª£c',\n",
    "    \n",
    "    # ƒê·∫°i t·ª´\n",
    "    'mik': 'm√¨nh', 'mk': 'm√¨nh', 'mjk': 'm√¨nh',\n",
    "    'vs': 'v·ªõi', 'v': 'v·∫≠y', 'z': 'v·∫≠y', 'vay': 'v·∫≠y',\n",
    "    \n",
    "    # T·ª´ th√¥ng d·ª•ng\n",
    "    'bik': 'bi·∫øt', 'bit': 'bi·∫øt', 'biet': 'bi·∫øt',\n",
    "    'wa': 'qu√°', 'w√°': 'qu√°', 'qua': 'qu√°',\n",
    "    'j': 'g√¨', 'gi': 'g√¨',\n",
    "    'lm': 'l√†m', 'lam': 'l√†m',\n",
    "    'ms': 'm·ªõi', 'moi': 'm·ªõi',\n",
    "    'r': 'r·ªìi', 'rui': 'r·ªìi', 'r√≤i': 'r·ªìi',\n",
    "    'cx': 'c≈©ng', 'cug': 'c≈©ng',\n",
    "    'th': 'th√¨', 'thi': 'th√¨',\n",
    "    \n",
    "    # Toxic (GI·ªÆ C·∫¢ HAI - quan tr·ªçng!)\n",
    "    'vcl': 'v√£i c·∫£ l·ªìn',\n",
    "    'vl': 'v√£i l·ªìn',\n",
    "    'dm': 'ƒë·ªãt m·∫π',\n",
    "    'dmm': 'ƒë·ªãt m·∫π m√†y',\n",
    "    'ƒëm': 'ƒë·ªãt m·∫π',\n",
    "    'cc': 'c·∫∑c',\n",
    "    'dcm': 'ƒë·ªãt con m·∫π',\n",
    "    'ƒëcm': 'ƒë·ªãt con m·∫π',\n",
    "    'clm': 'c√°i l·ªìn m·∫π',\n",
    "    'cmm': 'con m·∫π m√†y',\n",
    "    'cmn': 'con m·∫π n√≥',\n",
    "    'ƒë': 'ƒë√©o',\n",
    "    'wtf': 'what_the_fuck',\n",
    "    'loz': 'l·ªìn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b66caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr∆∞·ªõc: th·∫±ng ngu vcl dm\n",
      "Sau: th·∫±ng ngu vcl v√£i c·∫£ l·ªìn dm ƒë·ªãt m·∫π\n"
     ]
    }
   ],
   "source": [
    "def expand_teencode(text, teencode_dict):\n",
    "    \"\"\"\n",
    "    M·ªü r·ªông teencode - GI·ªÆ c·∫£ g·ªëc v√† full\n",
    "    V√≠ d·ª•: \"ngu vcl\" ‚Üí \"ngu vcl v√£i_c√°i_l·ªìn\"\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Lu√¥n gi·ªØ t·ª´ g·ªëc\n",
    "        expanded_words.append(word)\n",
    "        \n",
    "        # N·∫øu l√† teencode, th√™m t·ª´ ƒë·∫ßy ƒë·ªß\n",
    "        if word in teencode_dict:\n",
    "            full_word = teencode_dict[word]\n",
    "            expanded_words.append(full_word)\n",
    "    \n",
    "    return ' '.join(expanded_words)\n",
    "# Test\n",
    "test_text = \"th·∫±ng ngu vcl dm\"\n",
    "result = expand_teencode(test_text, teencode_dict)\n",
    "print(f\"Tr∆∞·ªõc: {test_text}\")\n",
    "print(f\"Sau: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2576dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ expand teencode\n"
     ]
    }
   ],
   "source": [
    "vietnamese_mask = df['language'] == 'vi'\n",
    "df.loc[vietnamese_mask, 'text'] = df.loc[vietnamese_mask, 'text'].apply(\n",
    "    lambda x: expand_teencode(x, teencode_dict)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ƒê√£ expand teencode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "414b229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KI·ªÇM TRA V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU (ƒê√É S·ª¨A L·ªñI)\n",
      "============================================================\n",
      "T·ªïng s·ªë documents ban ƒë·∫ßu: 164,064\n",
      "\n",
      "1. Ki·ªÉm tra null values: 0\n",
      "\n",
      "2. Ki·ªÉm tra empty strings: 2\n",
      "   ‚ö†Ô∏è T√¨m th·∫•y 2 empty strings!\n",
      "   üîß ƒêang lo·∫°i b·ªè...\n",
      "   ‚úÖ ƒê√£ lo·∫°i b·ªè. C√≤n l·∫°i: 164,062 documents\n",
      "\n",
      "3. Ki·ªÉm tra ƒë·ªô d√†i < 5 k√Ω t·ª±: 0\n",
      "\n",
      "‚úÖ D·ªØ li·ªáu ƒë√£ s·∫°ch!\n",
      "   ‚Ä¢ X_text: 164,062 documents\n",
      "   ‚Ä¢ y: 164,062 labels\n",
      "   ‚Ä¢ Toxic: 16,748 (10.2%)\n",
      "   ‚Ä¢ Non-toxic: 147,314 (89.8%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KI·ªÇM TRA V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU (ƒê√É S·ª¨A L·ªñI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# B·∫Øt ƒë·∫ßu v·ªõi df t·ª´ cell tr∆∞·ªõc (164,064 d√≤ng)\n",
    "print(f\"T·ªïng s·ªë documents ban ƒë·∫ßu: {len(df):,}\")\n",
    "\n",
    "# 1. Ki·ªÉm tra null/NaN \n",
    "null_count = df['text'].isnull().sum()\n",
    "print(f\"\\n1. Ki·ªÉm tra null values: {null_count}\")\n",
    "\n",
    "if null_count > 0:\n",
    "    print(f\"   ‚ö†Ô∏è T√¨m th·∫•y {null_count} null values!\")\n",
    "    print(f\"   üîß ƒêang lo·∫°i b·ªè...\")\n",
    "    # L·ªçc TR·ª∞C TI·∫æP tr√™n df\n",
    "    df = df[df['text'].notna()].copy()\n",
    "    print(f\"   ‚úÖ ƒê√£ lo·∫°i b·ªè. C√≤n l·∫°i: {len(df):,} documents\")\n",
    "\n",
    "# 2. Ki·ªÉm tra empty strings (t·ª´ c√°c b∆∞·ªõc x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥)\n",
    "empty_count = (df['text'].str.strip() == '').sum()\n",
    "print(f\"\\n2. Ki·ªÉm tra empty strings: {empty_count}\")\n",
    "\n",
    "if empty_count > 0:\n",
    "    print(f\"   ‚ö†Ô∏è T√¨m th·∫•y {empty_count} empty strings!\")\n",
    "    print(f\"   üîß ƒêang lo·∫°i b·ªè...\")\n",
    "    # L·ªçc TR·ª∞C TI·∫æP tr√™n df\n",
    "    valid_idx = df['text'].str.strip() != ''\n",
    "    df = df[valid_idx].copy()\n",
    "    print(f\"   ‚úÖ ƒê√£ lo·∫°i b·ªè. C√≤n l·∫°i: {len(df):,} documents\")\n",
    "\n",
    "# 3. Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu \n",
    "# ƒê·∫∑t m·ªôt ng∆∞·ª°ng t·ªëi thi·ªÉu, v√≠ d·ª• 5 k√Ω t·ª±\n",
    "MIN_LEN_FINAL = 5\n",
    "short_count = (df['text'].str.len() < MIN_LEN_FINAL).sum()\n",
    "print(f\"\\n3. Ki·ªÉm tra ƒë·ªô d√†i < {MIN_LEN_FINAL} k√Ω t·ª±: {short_count}\")\n",
    "\n",
    "if short_count > 0:\n",
    "    print(f\"   ‚ö†Ô∏è C√≥ {short_count} comments < {MIN_LEN_FINAL} k√Ω t·ª±\")\n",
    "    print(f\"   üîß ƒêang lo·∫°i b·ªè...\")\n",
    "    # L·ªçc TR·ª∞C TI·∫æP tr√™n df\n",
    "    valid_idx = df['text'].str.len() >= MIN_LEN_FINAL\n",
    "    df = df[valid_idx].copy()\n",
    "    print(f\"   ‚úÖ ƒê√£ lo·∫°i b·ªè. C√≤n l·∫°i: {len(df):,} documents\")\n",
    "\n",
    "# Reset index m·ªôt l·∫ßn DUY NH·∫§T sau khi ƒë√£ l·ªçc xong\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# G√°n X_text v√† y T·ª™ df ƒë√£ s·∫°ch\n",
    "X_text = df['text']\n",
    "y = df['label']\n",
    "\n",
    "print(f\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ s·∫°ch!\")\n",
    "print(f\"   ‚Ä¢ X_text: {len(X_text):,} documents\")\n",
    "print(f\"   ‚Ä¢ y: {len(y):,} labels\")\n",
    "print(f\"   ‚Ä¢ Toxic: {(y==1).sum():,} ({(y==1).mean()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Non-toxic: {(y==0).sum():,} ({(y==0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "647e6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count_null = df['text'].isnull().sum()\n",
    "print(count_null)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4a8401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u v√†o: ../data/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# L∆∞u d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch\n",
    "output_file = '../data/cleaned_dataset.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u v√†o: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07e9096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SO S√ÅNH TR∆Ø·ªöC V√Ä SAU X·ª¨ L√ù\n",
      "================================================================================\n",
      "\n",
      "üìä Th·ªëng k√™:\n",
      "   ‚Ä¢ T·ªïng s·ªë records g·ªëc: 164,069\n",
      "   ‚Ä¢ T·ªïng s·ªë records sau x·ª≠ l√Ω: 164,062\n",
      "   ‚Ä¢ S·ªë records b·ªã lo·∫°i: 7\n",
      "   ‚Ä¢ Ti·∫øng Vi·ªát trong cleaned: 4,496\n",
      "\n",
      "üîç T√åM C√ÅC M·∫™U C√ì THAY ƒê·ªîI L·ªöN:\n",
      "================================================================================\n",
      "\n",
      "T·ªïng s·ªë m·∫´u ti·∫øng Vi·ªát: 4,496\n",
      "\n",
      "üìù M·∫´u 1:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: youtube\n",
      "\n",
      "   üì• TR∆Ø·ªöC (27 k√Ω t·ª±):\n",
      "      M·∫∑t bu·ªìn th·∫•y c∆∞ng qu√° h√† üòÇ\n",
      "\n",
      "   üì§ SAU (25 k√Ω t·ª±):\n",
      "      m·∫∑t bu·ªìn th·∫•y c∆∞ng qu√° h√†\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 27 ‚Üí 25\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 2:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: youtube\n",
      "\n",
      "   üì• TR∆Ø·ªöC (22 k√Ω t·ª±):\n",
      "      K·∫ª ƒë·ªôc h√†nh ƒëi ad ∆°iii\n",
      "\n",
      "   üì§ SAU (58 k√Ω t·ª±):\n",
      "      haha b√† hi·ªÅn th·∫©m m·ªπ ki·ªÉu h√™nh l·∫Øm m·ªõi x·∫•u ƒëc ƒë∆∞·ª£c z v·∫≠y √°\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 22 ‚Üí 58\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 3:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: youtube\n",
      "\n",
      "   üì• TR∆Ø·ªöC (117 k√Ω t·ª±):\n",
      "      ƒê√∫ng l√† im l·∫∑ng ng·∫Øm tr√≤ v√† ƒë·ªùi!! T·ª•i n√≥ v√πi d·∫≠p 2 th·∫ßy tr√≤ k c√≥ ƒë∆∞·ªùng ch·ªëng ch·∫ø, v√† ƒë√¢y l√† c√°ch ƒë√°p tr·∫£, th∆∞∆°ng Binz\n",
      "\n",
      "   üì§ SAU (300 k√Ω t·ª±):\n",
      "      m√¨nh tua ƒëi tua l·∫°i b√†i c·ªßa binz nhi·ªÅu l·∫ßn ƒë·ªÉ nghe cho ƒë√£ lu√¥n. h√¥m nay bin ƒë√£ b√πng ch√°y h·∫øt n·ªói l√≤ng, t√¨nh c·∫£m c·ªßa m√¨nh d√†nh cho team & ƒë√°p tr·∫£ nh·ªØng...\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 117 ‚Üí 300\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 4:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: vnexpress\n",
      "\n",
      "   üì• TR∆Ø·ªöC (15 k√Ω t·ª±):\n",
      "      Ch√∫c m·ª´ng Saka!\n",
      "\n",
      "   üì§ SAU (65 k√Ω t·ª±):\n",
      "      c√≥ ai th·∫•y b·∫°n tien ƒë√¢u kh√¥ng, t√¨m m√£i s√°ng gi·ªù m√† ch∆∞a th·∫•y. :-)\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 15 ‚Üí 65\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 5:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: vnexpress\n",
      "\n",
      "   üì• TR∆Ø·ªöC (167 k√Ω t·ª±):\n",
      "      Nh·ªØng tr√≤ ch∆°i tr√≠ tu·ªá nh∆∞ c·ªù vua, c·ªù t∆∞·ªõng, c·ªù v√¢y...c·∫ßn ƒë∆∞·ª£c ph·ªï bi·∫øn r·ªông r√£i. Khi r·∫£nh r·ªói c√≥ th·ª© gi·∫£i tr√≠ gi·∫øt th·ªùi gian thay v√¨ lao v√†o c√°c cu·ªôc...\n",
      "\n",
      "   üì§ SAU (64 k√Ω t·ª±):\n",
      "      m·ªôt trong nh·ªØng n·ªói s·ª£ c·ªßa c√°c b√† v·ª£ l√† s·ª£ ch·ªìng m√™ c·ªù t∆∞·ªõng :-)\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 167 ‚Üí 64\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 6:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: youtube\n",
      "\n",
      "   üì• TR∆Ø·ªöC (63 k√Ω t·ª±):\n",
      "      Sammy ko lo v√¨ c√≤n th√™m antifan l√† ph·ª• huynh n·ªØaüóø (understand?)\n",
      "\n",
      "   üì§ SAU (68 k√Ω t·ª±):\n",
      "      sammy ko kh√¥ng lo v√¨ c√≤n th√™m antifan l√† ph·ª• huynh n·ªØa (understand?)\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 63 ‚Üí 68\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 7:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: vnexpress\n",
      "\n",
      "   üì• TR∆Ø·ªöC (24 k√Ω t·ª±):\n",
      "      Coi c√°i gi·∫£i thua v√¨ l√≠t\n",
      "\n",
      "   üì§ SAU (42 k√Ω t·ª±):\n",
      "      nh√† vua c·ªßa nh·ªØng tr·∫≠n giao h·ªØu ƒë√£ tr·ªü l·∫°i\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 24 ‚Üí 42\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìù M·∫´u 8:\n",
      "   Label: üü¢ Non-toxic\n",
      "   Source: youtube\n",
      "\n",
      "   üì• TR∆Ø·ªöC (45 k√Ω t·ª±):\n",
      "      B√¨nh lu·∫≠n ƒë·∫ßu n√® √† m√† video 0 view lu√¥n ch·ªõ:)\n",
      "\n",
      "   üì§ SAU (58 k√Ω t·ª±):\n",
      "      gi·ªù m·ªõi th·∫•y c√†ng gh√©t m√¨nh th√¨ ƒë·ªô th√†nh c√¥ng c√†ng cao =))\n",
      "\n",
      "   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\n",
      "      ‚úì Lowercase\n",
      "      ‚úì ƒê·ªô d√†i: 45 ‚Üí 58\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìà PH√ÇN T√çCH T·ªîNG QUAN:\n",
      "================================================================================\n",
      "   ‚Ä¢ C√≥ teencode expanded (ch·ª©a _): 0\n",
      "   ‚Ä¢ To√†n lowercase: 100\n",
      "   ‚Ä¢ C√≥ k√Ω t·ª± l·∫∑p ‚â•3 l·∫ßn: 0\n",
      "   ‚Ä¢ Trung b√¨nh ƒë·ªô d√†i: 105.5 k√Ω t·ª±\n",
      "\n",
      "‚úÖ Ho√†n th√†nh ph√¢n t√≠ch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11284\\2420621992.py:90: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  'C√≥ k√Ω t·ª± l·∫∑p ‚â•3 l·∫ßn': sample_texts.str.contains(r'(.)\\1{2,}', na=False).sum(),\n"
     ]
    }
   ],
   "source": [
    "# Load l·∫°i d·ªØ li·ªáu g·ªëc ƒë·ªÉ so s√°nh\n",
    "df_original = pd.read_csv('../data/merged_dataset.csv')\n",
    "df_cleaned = pd.read_csv('../data/cleaned_dataset.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SO S√ÅNH TR∆Ø·ªöC V√Ä SAU X·ª¨ L√ù\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Th·ªëng k√™:\")\n",
    "print(f\"   ‚Ä¢ T·ªïng s·ªë records g·ªëc: {len(df_original):,}\")\n",
    "print(f\"   ‚Ä¢ T·ªïng s·ªë records sau x·ª≠ l√Ω: {len(df_cleaned):,}\")\n",
    "print(f\"   ‚Ä¢ S·ªë records b·ªã lo·∫°i: {len(df_original) - len(df_cleaned):,}\")\n",
    "print(f\"   ‚Ä¢ Ti·∫øng Vi·ªát trong cleaned: {len(df_cleaned[df_cleaned['language'] == 'vi']):,}\")\n",
    "\n",
    "# T√¨m c√°c v√≠ d·ª• c√≥ s·ª± thay ƒë·ªïi r√µ r·ªát\n",
    "print(f\"\\nüîç T√åM C√ÅC M·∫™U C√ì THAY ƒê·ªîI L·ªöN:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# L·ªçc d·ªØ li·ªáu ti·∫øng Vi·ªát t·ª´ c·∫£ 2 dataset\n",
    "vi_original = df_original[df_original['language'] == 'vi'].reset_index(drop=True)\n",
    "vi_cleaned = df_cleaned[df_cleaned['language'] == 'vi'].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nT·ªïng s·ªë m·∫´u ti·∫øng Vi·ªát: {len(vi_cleaned):,}\")\n",
    "\n",
    "# So s√°nh 10 m·∫´u ng·∫´u nhi√™n\n",
    "samples = vi_cleaned.sample(10, random_state=42)\n",
    "\n",
    "changes_found = 0\n",
    "for i, (idx, row) in enumerate(samples.iterrows(), 1):\n",
    "    cleaned_text = row['text']\n",
    "    \n",
    "    # T√¨m text g·ªëc t∆∞∆°ng ·ª©ng (c√πng index sau reset)\n",
    "    if idx < len(vi_original):\n",
    "        original_text = vi_original.loc[idx, 'text']\n",
    "        \n",
    "        # Ch·ªâ hi·ªÉn th·ªã n·∫øu c√≥ thay ƒë·ªïi\n",
    "        if original_text.lower().strip() != cleaned_text.strip():\n",
    "            changes_found += 1\n",
    "            label = row['label']\n",
    "            source = row['source']\n",
    "            \n",
    "            print(f\"\\nüìù M·∫´u {changes_found}:\")\n",
    "            print(f\"   Label: {'üî¥ Toxic' if label == 1 else 'üü¢ Non-toxic'}\")\n",
    "            print(f\"   Source: {source}\")\n",
    "            print(f\"\\n   üì• TR∆Ø·ªöC ({len(original_text)} k√Ω t·ª±):\")\n",
    "            if len(original_text) > 150:\n",
    "                print(f\"      {original_text[:150]}...\")\n",
    "            else:\n",
    "                print(f\"      {original_text}\")\n",
    "            \n",
    "            print(f\"\\n   üì§ SAU ({len(cleaned_text)} k√Ω t·ª±):\")\n",
    "            if len(cleaned_text) > 150:\n",
    "                print(f\"      {cleaned_text[:150]}...\")\n",
    "            else:\n",
    "                print(f\"      {cleaned_text}\")\n",
    "            \n",
    "            # Ph√¢n t√≠ch thay ƒë·ªïi\n",
    "            changes = []\n",
    "            if original_text.lower() != original_text:\n",
    "                changes.append(\"‚úì Lowercase\")\n",
    "            if '@' in original_text and '@' not in cleaned_text:\n",
    "                changes.append(\"‚úì Lo·∫°i @mentions\")\n",
    "            if 'http' in original_text and 'http' not in cleaned_text:\n",
    "                changes.append(\"‚úì Lo·∫°i URLs\")\n",
    "            if '_' in cleaned_text:\n",
    "                changes.append(\"‚úì Expand teencode\")\n",
    "            if len(original_text) != len(cleaned_text):\n",
    "                changes.append(f\"‚úì ƒê·ªô d√†i: {len(original_text)} ‚Üí {len(cleaned_text)}\")\n",
    "            \n",
    "            if changes:\n",
    "                print(f\"\\n   üîß C√°c x·ª≠ l√Ω ƒë√£ √°p d·ª•ng:\")\n",
    "                for change in changes:\n",
    "                    print(f\"      {change}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "\n",
    "if changes_found == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y thay ƒë·ªïi r√µ r·ªát trong 10 m·∫´u ng·∫´u nhi√™n.\")\n",
    "    print(\"   C√≥ th·ªÉ do: index kh√¥ng t∆∞∆°ng ·ª©ng ho·∫∑c c√°c thay ƒë·ªïi nh·ªè\")\n",
    "\n",
    "# Ph√¢n t√≠ch t·ªïng quan\n",
    "print(\"\\nüìà PH√ÇN T√çCH T·ªîNG QUAN:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_texts = df_cleaned[df_cleaned['language'] == 'vi']['text'].head(100)\n",
    "\n",
    "patterns_found = {\n",
    "    'C√≥ teencode expanded (ch·ª©a _)': sample_texts.str.contains('_', na=False).sum(),\n",
    "    'To√†n lowercase': sample_texts.str.islower().sum(),\n",
    "    'C√≥ k√Ω t·ª± l·∫∑p ‚â•3 l·∫ßn': sample_texts.str.contains(r'(.)\\1{2,}', na=False).sum(),\n",
    "    'Trung b√¨nh ƒë·ªô d√†i': f\"{sample_texts.str.len().mean():.1f} k√Ω t·ª±\"\n",
    "}\n",
    "\n",
    "for key, value in patterns_found.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n th√†nh ph√¢n t√≠ch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2983a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
